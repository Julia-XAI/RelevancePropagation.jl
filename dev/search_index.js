var documenterSearchIndex = {"docs":
[{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"EditURL = \"../literate/custom_layer.jl\"","category":"page"},{"location":"generated/custom_layer/#custom-layers","page":"Supporting New Layer Types","title":"Supporting New Layers and Activation Functions","text":"","category":"section"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"One of the design goals of RelevancePropagation.jl is to combine ease of use and extensibility for the purpose of research. This example will show you how to extent LRP to new layer types and activation functions.","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"using Flux\nusing RelevancePropagation","category":"page"},{"location":"generated/custom_layer/#model-checks","page":"Supporting New Layer Types","title":"Model checks","text":"","category":"section"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"To assure that novice users use LRP according to best practices, RelevancePropagation.jl runs strict model checks when creating an LRP analyzer.","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"Let's demonstrate this by defining a new layer type that doubles its input","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"struct MyDoublingLayer end\n(::MyDoublingLayer)(x) = 2 * x\n\nmylayer = MyDoublingLayer()\nmylayer([1, 2, 3])","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"and by defining a model that uses this layer:","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"model = Chain(Dense(100, 20), MyDoublingLayer());\nnothing #hide","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"Creating an LRP analyzer, e.g. LRP(model), will throw an ArgumentError and print a summary of the model check in the REPL:","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"julia> LRP(model)\n  ChainTuple(\n    Dense(100 => 20)  => supported,\n    MyDoublingLayer() => unknown layer type,\n  ),\n\n  LRP model check failed\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found unknown layer types or activation functions that are not supported by RelevancePropagation's LRP implementation yet.\n\n  LRP assumes that the model is a deep rectifier network that only contains ReLU-like activation functions.\n\n  If you think the missing layer should be supported by default, please submit an issue (https://github.com/Julia-XAI/RelevancePropagation.jl/issues).\n\n  [...]\n\nERROR: Unknown layer or activation function found in model","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"LRP should only be used on deep rectifier networks and RelevancePropagation doesn't recognize MyDoublingLayer as a compatible layer by default. It will therefore return an error and a model check summary instead of returning an incorrect explanation.","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"However, if we know MyDoublingLayer is compatible with deep rectifier networks, we can register it to tell RelevancePropagation that it is ok to use. This will be shown in the following section.","category":"page"},{"location":"generated/custom_layer/#Registering-layers","page":"Supporting New Layer Types","title":"Registering layers","text":"","category":"section"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"The error in the model check will stop after registering our custom layer type MyDoublingLayer as \"supported\" by RelevancePropagation.","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"This is done using the function LRP_CONFIG.supports_layer, which should be set to return true for the type MyDoublingLayer:","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"LRP_CONFIG.supports_layer(::MyDoublingLayer) = true","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"Now we can create and run an analyzer without getting an error:","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"analyzer = LRP(model)","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"note: Registering functions\nFlux's Chains can also contain functions, e.g. flatten. This kind of layer can be registered asLRP_CONFIG.supports_layer(::typeof(flatten)) = true","category":"page"},{"location":"generated/custom_layer/#Registering-activation-functions","page":"Supporting New Layer Types","title":"Registering activation functions","text":"","category":"section"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"The mechanism for registering custom activation functions is analogous to that of custom layers:","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"myrelu(x) = max.(0, x)\n\nmodel = Chain(Dense(784, 100, myrelu), Dense(100, 10));\nnothing #hide","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"Once again, creating an LRP analyzer for this model will throw an ArgumentError and display the following model check summary:","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"julia> LRP(model)\n  ChainTuple(\n    Dense(784 => 100, myrelu) => unsupported or unknown activation function myrelu,\n    Dense(100 => 10)          => supported,\n  ),\n\n  LRP model check failed\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found unknown layer types or activation functions that are not supported by RelevancePropagation's LRP implementation yet.\n\n  LRP assumes that the model is a deep rectifier network that only contains ReLU-like activation functions.\n\n  If you think the missing layer should be supported by default, please submit an issue (https://github.com/Julia-XAI/RelevancePropagation.jl/issues).\n\n  [...]\n\nERROR: Unknown layer or activation function found in model","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"Registation works by defining the function LRP_CONFIG.supports_activation as true:","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"LRP_CONFIG.supports_activation(::typeof(myrelu)) = true","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"now the analyzer can be created without error:","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"analyzer = LRP(model)","category":"page"},{"location":"generated/custom_layer/#Skipping-model-checks","page":"Supporting New Layer Types","title":"Skipping model checks","text":"","category":"section"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"All model checks can be skipped at your own risk by setting the LRP-analyzer keyword argument skip_checks=true.","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"struct UnknownLayer end\n(::UnknownLayer)(x) = x\n\nunknown_activation(x) = max.(0, x)\n\nmodel = Chain(Dense(100, 20, unknown_activation), MyDoublingLayer())\n\nLRP(model; skip_checks=true)","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"Instead of throwing the usual ERROR: Unknown layer or activation function found in model, the LRP analyzer was created without having to register either the layer UnknownLayer or the activation function unknown_activation.","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"","category":"page"},{"location":"generated/custom_layer/","page":"Supporting New Layer Types","title":"Supporting New Layer Types","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Basic-API","page":"API Reference","title":"Basic API","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"All methods in RelevancePropagation.jl work by calling analyze on an input and an analyzer:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"analyze\nExplanation\nheatmap","category":"page"},{"location":"api/#XAIBase.analyze","page":"API Reference","title":"XAIBase.analyze","text":"analyze(input, method)\nanalyze(input, method, neuron_selection)\n\nApply the analyzer method for the given input, returning an Explanation. If neuron_selection is specified, e.g. the index of a specific output neuron, the explanation will be calculated for that neuron. Otherwise, the output neuron with the highest activation is automatically chosen.\n\nSee also Explanation and heatmap.\n\nKeyword arguments\n\nadd_batch_dim: add batch dimension to the input without allocating. Default is false.\n\n\n\n\n\n","category":"function"},{"location":"api/#XAIBase.Explanation","page":"API Reference","title":"XAIBase.Explanation","text":"Return type of analyzers when calling analyze.\n\nFields\n\nval: numerical output of the analyzer, e.g. an attribution or gradient\noutput: model output for the given analyzer input\noutput_selection: index of the output used for the explanation\nanalyzer: symbol corresponding the used analyzer, e.g. :Gradient or :LRP\nheatmap: symbol indicating a preset heatmapping style,   e.g. :attibution, :sensitivity or :cam\nextras: optional named tuple that can be used by analyzers   to return additional information.\n\n\n\n\n\n","category":"type"},{"location":"api/#XAIBase.heatmap","page":"API Reference","title":"XAIBase.heatmap","text":"heatmap(explanation)\n\nVisualize Explanation from XAIBase as a vision heatmap. Assumes WHCN convention (width, height, channels, batchsize) for explanation.val.\n\nKeyword arguments\n\ncolorscheme::Union{ColorScheme,Symbol}: color scheme from ColorSchemes.jl. Defaults to :seismic.\nreduce::Symbol: Selects how color channels are reduced to a single number to apply a color scheme. The following methods can be selected, which are then applied over the color channels for each \"pixel\" in the array:\n:sum: sum up color channels\n:norm: compute 2-norm over the color channels\n:maxabs: compute maximum(abs, x) over the color channels\nDefaults to :sum.\nrangescale::Symbol: Selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either :extrema or :centered. Defaults to :centered.\nprocess_batch::Bool: When heatmapping a batch, setting process_batch=true will apply the rangescale normalization to the entire batch instead of computing it individually for each sample in the batch. Defaults to false.\npermute::Bool: Whether to flip W&H input channels. Default is true.\nunpack_singleton::Bool: If false, heatmap will always return a vector of images. When heatmapping a batch with a single sample, setting unpack_singleton=true will unpack the singleton vector and directly return the image. Defaults to true.\n\n\n\n\n\nheatmap(input, analyzer)\n\nCompute an Explanation for a given input using the method analyzer and visualize it as a vision heatmap.\n\nAny additional arguments and keyword arguments are passed to the analyzer. Refer to analyze for more information on available keyword arguments.\n\nTo customize the heatmapping style, first compute an explanation using analyze and then call heatmap on the explanation.\n\n\n\n\n\nheatmap(explanation, text)\n\nVisualize Explanation from XAIBase as text heatmap. Text should be a vector containing vectors of strings, one for each input in the batched explanation.\n\nKeyword arguments\n\ncolorscheme::Union{ColorScheme,Symbol}: color scheme from ColorSchemes.jl. Defaults to :seismic.\nrangescale::Symbol: selects how the color channel reduced heatmap is normalized before the color scheme is applied. Can be either :extrema or :centered. Defaults to :centered for use with the default color scheme :seismic.\n\n\n\n\n\n","category":"function"},{"location":"api/#LRP-analyzer","page":"API Reference","title":"LRP analyzer","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"LRP","category":"page"},{"location":"api/#RelevancePropagation.LRP","page":"API Reference","title":"RelevancePropagation.LRP","text":"LRP(model, rules)\nLRP(model, composite)\n\nAnalyze model by applying Layer-Wise Relevance Propagation. The analyzer can either be created by passing an array of LRP-rules or by passing a composite, see Composite for an example.\n\nKeyword arguments\n\nskip_checks::Bool: Skip checks whether model is compatible with LRP and contains output softmax. Default is false.\nverbose::Bool: Select whether the model checks should print a summary on failure. Default is true.\n\nReferences\n\n[1] G. Montavon et al., Layer-Wise Relevance Propagation: An Overview [2] W. Samek et al., Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications\n\n\n\n\n\n","category":"type"},{"location":"api/#Model-preparation","page":"API Reference","title":"Model preparation","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"strip_softmax\ncanonize\nflatten_model","category":"page"},{"location":"api/#RelevancePropagation.strip_softmax","page":"API Reference","title":"RelevancePropagation.strip_softmax","text":"strip_softmax(model)\nstrip_softmax(layer)\n\nRemove softmax activation on layer or model if it exists.\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.canonize","page":"API Reference","title":"RelevancePropagation.canonize","text":"canonize(model)\n\nCanonize model by flattening it and fusing BatchNorm layers into preceding Dense and Conv layers with linear activation functions.\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.flatten_model","page":"API Reference","title":"RelevancePropagation.flatten_model","text":"flatten_model(model)\n\nFlatten a Flux Chain containing Chains.\n\n\n\n\n\n","category":"function"},{"location":"api/#LRP-rules","page":"API Reference","title":"LRP rules","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Refer to the LRP rule overview for a detailed explanation  of the notation used for LRP rules.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"ZeroRule\nEpsilonRule\nGammaRule\nWSquareRule\nFlatRule\nAlphaBetaRule\nZPlusRule\nZBoxRule\nPassRule\nGeneralizedGammaRule\nLayerNormRule","category":"page"},{"location":"api/#RelevancePropagation.ZeroRule","page":"API Reference","title":"RelevancePropagation.ZeroRule","text":"ZeroRule()\n\nLRP-0 rule. Commonly used on upper layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_i fracW_ija_j^ksum_l W_ila_l^k+b_i R_i^k+1\n\nReferences\n\nS. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.EpsilonRule","page":"API Reference","title":"RelevancePropagation.EpsilonRule","text":"EpsilonRule([epsilon=1.0e-6])\n\nLRP-ϵ rule. Commonly used on middle layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifracW_ija_j^kepsilon +sum_lW_ila_l^k+b_i R_i^k+1\n\nOptional arguments\n\nepsilon: Optional stabilization parameter, defaults to 1.0e-6.\n\nReferences\n\nS. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.GammaRule","page":"API Reference","title":"RelevancePropagation.GammaRule","text":"GammaRule([gamma=0.25])\n\nLRP-γ rule. Commonly used on lower layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifrac(W_ij+gamma W_ij^+)a_j^k\n    sum_l(W_il+gamma W_il^+)a_l^k+(b_i+gamma b_i^+) R_i^k+1\n\nOptional arguments\n\ngamma: Optional multiplier for added positive weights, defaults to 0.25.\n\nReferences\n\nG. Montavon et al., Layer-Wise Relevance Propagation: An Overview\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.WSquareRule","page":"API Reference","title":"RelevancePropagation.WSquareRule","text":"WSquareRule()\n\nLRP-w² rule. Commonly used on the first layer when values are unbounded.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifracW_ij^2sum_l W_il^2 R_i^k+1\n\nReferences\n\nG. Montavon et al., Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.FlatRule","page":"API Reference","title":"RelevancePropagation.FlatRule","text":"FlatRule()\n\nLRP-Flat rule. Similar to the WSquareRule, but with all weights set to one and all bias terms set to zero.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifrac1sum_l 1 R_i^k+1 = sum_ifrac1n_i R_i^k+1\n\nwhere n_i is the number of input neurons connected to the output neuron at index i.\n\nReferences\n\nS. Lapuschkin et al., Unmasking Clever Hans predictors and assessing what machines really learn\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.AlphaBetaRule","page":"API Reference","title":"RelevancePropagation.AlphaBetaRule","text":"AlphaBetaRule([alpha=2.0, beta=1.0])\n\nLRP-αβ rule. Weights positive and negative contributions according to the parameters alpha and beta respectively. The difference α-β must be equal to one. Commonly used on lower layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ileft(\n    alphafracleft(W_ija_j^kright)^+sum_lleft(W_ila_l^k+b_iright)^+\n    -betafracleft(W_ija_j^kright)^-sum_lleft(W_ila_l^k+b_iright)^-\nright) R_i^k+1\n\nOptional arguments\n\nalpha: Multiplier for the positive output term, defaults to 2.0.\nbeta: Multiplier for the negative output term, defaults to 1.0.\n\nReferences\n\nS. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation\nG. Montavon et al., Layer-Wise Relevance Propagation: An Overview\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.ZPlusRule","page":"API Reference","title":"RelevancePropagation.ZPlusRule","text":"ZPlusRule()\n\nLRP-z rule. Commonly used on lower layers.\n\nEquivalent to AlphaBetaRule(1.0f0, 0.0f0), but slightly faster. See also AlphaBetaRule.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifracleft(W_ija_j^kright)^+sum_lleft(W_ila_l^k+b_iright)^+ R_i^k+1\n\nReferences\n\nS. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation\nG. Montavon et al., Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.ZBoxRule","page":"API Reference","title":"RelevancePropagation.ZBoxRule","text":"ZBoxRule(low, high)\n\nLRP-zᴮ-rule. Commonly used on the first layer for pixel input.\n\nThe parameters low and high should be set to the lower and upper bounds of the input features, e.g. 0.0 and 1.0 for raw image data. It is also possible to provide two arrays of that match the input size.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k=sum_i fracW_ija_j^k - W_ij^+l_j - W_ij^-h_j\n    sum_l W_ila_l^k+b_i - left(W_il^+l_l+b_i^+right) - left(W_il^-h_l+b_i^-right) R_i^k+1\n\nReferences\n\nG. Montavon et al., Layer-Wise Relevance Propagation: An Overview\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.PassRule","page":"API Reference","title":"RelevancePropagation.PassRule","text":"PassRule()\n\nPass-through rule. Passes relevance through to the lower layer.\n\nSupports layers with constant input and output shapes, e.g. reshaping layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = R_j^k+1\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.GeneralizedGammaRule","page":"API Reference","title":"RelevancePropagation.GeneralizedGammaRule","text":"GeneralizedGammaRule([gamma=0.25])\n\nGeneralized LRP-γ rule. Can be used on layers with leakyrelu activation functions.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifrac\n    (W_ij+gamma W_ij^+)a_j^+ +(W_ij+gamma W_ij^-)a_j^-\n    sum_l(W_il+gamma W_il^+)a_j^+ +(W_il+gamma W_il^-)a_j^- +(b_i+gamma b_i^+)\nI(z_k0) cdot R^k+1_i\n+sum_ifrac\n    (W_ij+gamma W_ij^-)a_j^+ +(W_ij+gamma W_ij^+)a_j^-\n    sum_l(W_il+gamma W_il^-)a_j^+ +(W_il+gamma W_il^+)a_j^- +(b_i+gamma b_i^-)\nI(z_k0) cdot R^k+1_i\n\nOptional arguments\n\ngamma: Optional multiplier for added positive weights, defaults to 0.25.\n\nReferences\n\nL. Andéol et al., Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.LayerNormRule","page":"API Reference","title":"RelevancePropagation.LayerNormRule","text":"LayerNormRule()\n\nLRP-LN rule. Used on LayerNorm layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_i^k = sum_jfraca_i^kleft(delta_ij - 1Nright)sum_l a_l^kleft(delta_lj-1Nright) R_j^k+1\n\nRelevance through the affine transformation is by default propagated using the ZeroRule.\n\nIf you would like to assign a special rule to the affine transformation inside of the LayerNorm layer, call canonize on your model. This will split the LayerNorm layer into\n\na LayerNorm layer without affine transformation\na Scale layer implementing the affine transformation\n\nYou can then assign separate rules to these two layers.\n\nReferences\n\nA. Ali et al., XAI for Transformers: Better Explanations through Conservative Propagation\n\n\n\n\n\n","category":"type"},{"location":"api/#Composites","page":"API Reference","title":"Composites","text":"","category":"section"},{"location":"api/#Applying-composites","page":"API Reference","title":"Applying composites","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Composite\nlrp_rules","category":"page"},{"location":"api/#RelevancePropagation.Composite","page":"API Reference","title":"RelevancePropagation.Composite","text":"Composite(primitives...)\nComposite(default_rule, primitives...)\n\nAutomatically contructs a list of LRP-rules by sequentially applying composite primitives.\n\nPrimitives\n\nTo apply a single rule, use:\n\nLayerMap to apply a rule to the n-th layer of a model\nGlobalMap to apply a rule to all layers\nRangeMap to apply a rule to a positional range of layers\nFirstLayerMap to apply a rule to the first layer\nLastLayerMap to apply a rule to the last layer\n\nTo apply a set of rules to layers based on their type, use:\n\nGlobalTypeMap to apply a dictionary that maps layer types to LRP-rules\nRangeTypeMap for a TypeMap on generalized ranges\nFirstLayerTypeMap for a TypeMap on the first layer of a model\nLastLayerTypeMap for a TypeMap on the last layer\nFirstNTypeMap for a TypeMap on the first n layers\n\nExample\n\nUsing a VGG11 model:\n\njulia> composite = Composite(\n           GlobalTypeMap(\n               ConvLayer => AlphaBetaRule(),\n               Dense => EpsilonRule(),\n               PoolingLayer => EpsilonRule(),\n               DropoutLayer => PassRule(),\n               ReshapingLayer => PassRule(),\n           ),\n           FirstNTypeMap(7, Conv => FlatRule()),\n       );\n\njulia> analyzer = LRP(model, composite)\nLRP(\n  Conv((3, 3), 3 => 64, relu, pad=1)    => FlatRule(),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 64 => 128, relu, pad=1)  => FlatRule(),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 128 => 256, relu, pad=1) => FlatRule(),\n  Conv((3, 3), 256 => 256, relu, pad=1) => FlatRule(),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 256 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  Conv((3, 3), 512 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  Conv((3, 3), 512 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  Conv((3, 3), 512 => 512, relu, pad=1) => AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n  MaxPool((2, 2))                       => EpsilonRule{Float32}(1.0f-6),\n  MLUtils.flatten                       => PassRule(),\n  Dense(25088 => 4096, relu)            => EpsilonRule{Float32}(1.0f-6),\n  Dropout(0.5)                          => PassRule(),\n  Dense(4096 => 4096, relu)             => EpsilonRule{Float32}(1.0f-6),\n  Dropout(0.5)                          => PassRule(),\n  Dense(4096 => 1000)                   => EpsilonRule{Float32}(1.0f-6),\n)\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.lrp_rules","page":"API Reference","title":"RelevancePropagation.lrp_rules","text":"lrp_rules(model, composite)\n\nApply a composite to obtain LRP-rules for a given Flux model.\n\n\n\n\n\n","category":"function"},{"location":"api/#api-composite-primitives","page":"API Reference","title":"Composite primitives","text":"","category":"section"},{"location":"api/#Mapping-layers-to-rules","page":"API Reference","title":"Mapping layers to rules","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Composite primitives that apply a single rule:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"LayerMap\nGlobalMap\nRangeMap\nFirstLayerMap\nLastLayerMap","category":"page"},{"location":"api/#RelevancePropagation.LayerMap","page":"API Reference","title":"RelevancePropagation.LayerMap","text":"LayerMap(index, rule)\n\nComposite primitive that maps an LRP-rule to all layers in the model at the given index. The index can either be an integer or a tuple of integers to map a rule to a specific layer in nested Flux Chains.\n\nSee show_layer_indices to print layer indices and Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.GlobalMap","page":"API Reference","title":"RelevancePropagation.GlobalMap","text":"GlobalMap(rule)\n\nComposite primitive that maps an LRP-rule to all layers in the model.\n\nSee Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.RangeMap","page":"API Reference","title":"RelevancePropagation.RangeMap","text":"RangeMap(range, rule)\n\nComposite primitive that maps an LRP-rule to the specified positional range of layers in the model.\n\nSee Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.FirstLayerMap","page":"API Reference","title":"RelevancePropagation.FirstLayerMap","text":"FirstLayerMap(rule)\n\nComposite primitive that maps an LRP-rule to the first layer in the model.\n\nSee Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.LastLayerMap","page":"API Reference","title":"RelevancePropagation.LastLayerMap","text":"LastLayerMap(rule)\n\nComposite primitive that maps an LRP-rule to the last layer in the model.\n\nSee Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API Reference","title":"API Reference","text":"To apply LayerMap to nested Flux Chains or Parallel layers,  make use of show_layer_indices:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"show_layer_indices","category":"page"},{"location":"api/#RelevancePropagation.show_layer_indices","page":"API Reference","title":"RelevancePropagation.show_layer_indices","text":"show_layer_indices(model)\n\nPrint layer indices of Flux models. This is primarily a utility to help define LayerMap primitives.\n\n\n\n\n\n","category":"function"},{"location":"api/#Mapping-layers-to-rules-based-on-type","page":"API Reference","title":"Mapping layers to rules based on type","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Composite primitives that apply rules based on the layer type:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"GlobalTypeMap\nRangeTypeMap\nFirstLayerTypeMap\nLastLayerTypeMap\nFirstNTypeMap","category":"page"},{"location":"api/#RelevancePropagation.GlobalTypeMap","page":"API Reference","title":"RelevancePropagation.GlobalTypeMap","text":"GlobalTypeMap(map)\n\nComposite primitive that maps layer types to LRP rules based on a list of type-rule-pairs map.\n\nSee Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.RangeTypeMap","page":"API Reference","title":"RelevancePropagation.RangeTypeMap","text":"RangeTypeMap(range, map)\n\nComposite primitive that maps layer types to LRP rules based on a list of type-rule-pairs map within the specified range of layers in the model.\n\nSee Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.FirstLayerTypeMap","page":"API Reference","title":"RelevancePropagation.FirstLayerTypeMap","text":"FirstLayerTypeMap(map)\n\nComposite primitive that maps the type of the first layer of the model to LRP rules based on a list of type-rule-pairs map.\n\nSee Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.LastLayerTypeMap","page":"API Reference","title":"RelevancePropagation.LastLayerTypeMap","text":"LastLayerTypeMap(map)\n\nComposite primitive that maps the type of the last layer of the model to LRP rules based on a list of type-rule-pairs map.\n\nSee Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.FirstNTypeMap","page":"API Reference","title":"RelevancePropagation.FirstNTypeMap","text":"FirstNTypeMap(n, map)\n\nComposite primitive that maps layer types to LRP rules based on a list of type-rule-pairs map within the first n layers in the model.\n\nSee Composite for an example.\n\n\n\n\n\n","category":"type"},{"location":"api/#Union-types-for-composites","page":"API Reference","title":"Union types for composites","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"The following exported union types types can be used to define TypeMaps:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"ConvLayer\nPoolingLayer\nDropoutLayer\nReshapingLayer\nNormalizationLayer","category":"page"},{"location":"api/#RelevancePropagation.ConvLayer","page":"API Reference","title":"RelevancePropagation.ConvLayer","text":"Union type for convolutional layers.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.PoolingLayer","page":"API Reference","title":"RelevancePropagation.PoolingLayer","text":"Union type for pooling layers.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.DropoutLayer","page":"API Reference","title":"RelevancePropagation.DropoutLayer","text":"Union type for dropout layers.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.ReshapingLayer","page":"API Reference","title":"RelevancePropagation.ReshapingLayer","text":"Union type for reshaping layers such as flatten.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.NormalizationLayer","page":"API Reference","title":"RelevancePropagation.NormalizationLayer","text":"Union type for normalization layers.\n\n\n\n\n\n","category":"type"},{"location":"api/#api-composite-presets","page":"API Reference","title":"Composite presets","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"EpsilonGammaBox\nEpsilonPlus\nEpsilonAlpha2Beta1\nEpsilonPlusFlat\nEpsilonAlpha2Beta1Flat","category":"page"},{"location":"api/#RelevancePropagation.EpsilonGammaBox","page":"API Reference","title":"RelevancePropagation.EpsilonGammaBox","text":"EpsilonGammaBox(low, high; [epsilon=1.0f-6, gamma=0.25f0])\n\nComposite using the following primitives:\n\njulia> EpsilonGammaBox(-3.0f0, 3.0f0)\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.GammaRule{Float32}(0.25f0),\n    Flux.ConvTranspose      => RelevancePropagation.GammaRule{Float32}(0.25f0),\n    Flux.CrossCor           => RelevancePropagation.GammaRule{Float32}(0.25f0),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n  FirstLayerTypeMap(  # first layer\n    Flux.Conv          => RelevancePropagation.ZBoxRule{Float32}(-3.0f0, 3.0f0),\n    Flux.ConvTranspose => RelevancePropagation.ZBoxRule{Float32}(-3.0f0, 3.0f0),\n    Flux.CrossCor      => RelevancePropagation.ZBoxRule{Float32}(-3.0f0, 3.0f0),\n ),\n)\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.EpsilonPlus","page":"API Reference","title":"RelevancePropagation.EpsilonPlus","text":"EpsilonPlus(; [epsilon=1.0f-6])\n\nComposite using the following primitives:\n\njulia> EpsilonPlus()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.ZPlusRule(),\n    Flux.ConvTranspose      => RelevancePropagation.ZPlusRule(),\n    Flux.CrossCor           => RelevancePropagation.ZPlusRule(),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n)\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.EpsilonAlpha2Beta1","page":"API Reference","title":"RelevancePropagation.EpsilonAlpha2Beta1","text":"EpsilonAlpha2Beta1(; [epsilon=1.0f-6])\n\nComposite using the following primitives:\n\njulia> EpsilonAlpha2Beta1()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.ConvTranspose      => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.CrossCor           => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n)\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.EpsilonPlusFlat","page":"API Reference","title":"RelevancePropagation.EpsilonPlusFlat","text":"EpsilonPlusFlat(; [epsilon=1.0f-6])\n\nComposite using the following primitives:\n\njulia> EpsilonPlusFlat()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.ZPlusRule(),\n    Flux.ConvTranspose      => RelevancePropagation.ZPlusRule(),\n    Flux.CrossCor           => RelevancePropagation.ZPlusRule(),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n  FirstLayerTypeMap(  # first layer\n    Flux.Conv          => RelevancePropagation.FlatRule(),\n    Flux.ConvTranspose => RelevancePropagation.FlatRule(),\n    Flux.CrossCor      => RelevancePropagation.FlatRule(),\n ),\n)\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.EpsilonAlpha2Beta1Flat","page":"API Reference","title":"RelevancePropagation.EpsilonAlpha2Beta1Flat","text":"EpsilonAlpha2Beta1Flat(; [epsilon=1.0f-6])\n\nComposite using the following primitives:\n\njulia> EpsilonAlpha2Beta1Flat()\nComposite(\n  GlobalTypeMap(  # all layers\n    Flux.Conv               => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.ConvTranspose      => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.CrossCor           => RelevancePropagation.AlphaBetaRule{Float32}(2.0f0, 1.0f0),\n    Flux.Dense              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.Scale              => RelevancePropagation.EpsilonRule{Float32}(1.0f-6),\n    Flux.LayerNorm          => RelevancePropagation.LayerNormRule(),\n    typeof(NNlib.dropout)   => RelevancePropagation.PassRule(),\n    Flux.AlphaDropout       => RelevancePropagation.PassRule(),\n    Flux.Dropout            => RelevancePropagation.PassRule(),\n    Flux.BatchNorm          => RelevancePropagation.PassRule(),\n    typeof(Flux.flatten)    => RelevancePropagation.PassRule(),\n    typeof(MLUtils.flatten) => RelevancePropagation.PassRule(),\n    typeof(identity)        => RelevancePropagation.PassRule(),\n ),\n  FirstLayerTypeMap(  # first layer\n    Flux.Conv          => RelevancePropagation.FlatRule(),\n    Flux.ConvTranspose => RelevancePropagation.FlatRule(),\n    Flux.CrossCor      => RelevancePropagation.FlatRule(),\n ),\n)\n\n\n\n\n\n","category":"function"},{"location":"api/#Manual-rule-assignment","page":"API Reference","title":"Manual rule assignment","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"For manual rule assignment, use ChainTuple,  ParallelTuple and SkipConnectionTuple, matching the model structure:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"ChainTuple\nParallelTuple\nSkipConnectionTuple","category":"page"},{"location":"api/#RelevancePropagation.ChainTuple","page":"API Reference","title":"RelevancePropagation.ChainTuple","text":"ChainTuple(xs)\n\nThin wrapper around Tuple for use with Flux.jl models.\n\nCombining ChainTuple, ParallelTuple and SkipConnectionTuple, data xs can be stored while preserving the structure of a Flux model without risking type piracy.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.ParallelTuple","page":"API Reference","title":"RelevancePropagation.ParallelTuple","text":"ParallelTuple(xs)\n\nThin wrapper around Tuple for use with Flux.jl models.\n\nCombining ChainTuple, ParallelTuple and SkipConnectionTuple, data xs can be stored while preserving the structure of a Flux model without risking type piracy.\n\n\n\n\n\n","category":"type"},{"location":"api/#RelevancePropagation.SkipConnectionTuple","page":"API Reference","title":"RelevancePropagation.SkipConnectionTuple","text":"SkipConnectionTuple(xs)\n\nThin wrapper around Tuple for use with Flux.jl models.\n\nCombining ChainTuple, ParallelTuple and SkipConnectionTuple, data xs can be stored while preserving the structure of a Flux model without risking type piracy.\n\n\n\n\n\n","category":"type"},{"location":"api/#Custom-rules","page":"API Reference","title":"Custom rules","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"These utilities can be used to define custom rules without writing boilerplate code. To extend these functions, explicitly import them: ","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"RelevancePropagation.modify_input\nRelevancePropagation.modify_denominator\nRelevancePropagation.modify_parameters\nRelevancePropagation.modify_weight\nRelevancePropagation.modify_bias\nRelevancePropagation.modify_layer\nRelevancePropagation.is_compatible","category":"page"},{"location":"api/#RelevancePropagation.modify_input","page":"API Reference","title":"RelevancePropagation.modify_input","text":"modify_input(rule, input)\n\nModify input activation before computing relevance propagation.\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.modify_denominator","page":"API Reference","title":"RelevancePropagation.modify_denominator","text":"modify_denominator(rule, d)\n\nModify denominator z for numerical stability on the forward pass.\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.modify_parameters","page":"API Reference","title":"RelevancePropagation.modify_parameters","text":"modify_parameters(rule, parameter)\n\nModify parameters before computing the relevance.\n\nNote\n\nUse of a custom function modify_layer will overwrite functionality of modify_parameters, modify_weight and modify_bias for the implemented combination of rule and layer types. This is due to the fact that internally, modify_weight and modify_bias are called by the default implementation of modify_layer. modify_weight and modify_bias in turn call modify_parameters by default.\n\nThe default call structure looks as follows:\n\n┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.modify_weight","page":"API Reference","title":"RelevancePropagation.modify_weight","text":"modify_weight(rule, weight)\n\nModify layer weights before computing the relevance.\n\nNote\n\nUse of a custom function modify_layer will overwrite functionality of modify_parameters, modify_weight and modify_bias for the implemented combination of rule and layer types. This is due to the fact that internally, modify_weight and modify_bias are called by the default implementation of modify_layer. modify_weight and modify_bias in turn call modify_parameters by default.\n\nThe default call structure looks as follows:\n\n┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.modify_bias","page":"API Reference","title":"RelevancePropagation.modify_bias","text":"modify_bias(rule, bias)\n\nModify layer bias before computing the relevance.\n\nNote\n\nUse of a custom function modify_layer will overwrite functionality of modify_parameters, modify_weight and modify_bias for the implemented combination of rule and layer types. This is due to the fact that internally, modify_weight and modify_bias are called by the default implementation of modify_layer. modify_weight and modify_bias in turn call modify_parameters by default.\n\nThe default call structure looks as follows:\n\n┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.modify_layer","page":"API Reference","title":"RelevancePropagation.modify_layer","text":"modify_layer(rule, layer)\n\nModify layer before computing the relevance.\n\nNote\n\nUse of a custom function modify_layer will overwrite functionality of modify_parameters, modify_weight and modify_bias for the implemented combination of rule and layer types. This is due to the fact that internally, modify_weight and modify_bias are called by the default implementation of modify_layer. modify_weight and modify_bias in turn call modify_parameters by default.\n\nThe default call structure looks as follows:\n\n┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.is_compatible","page":"API Reference","title":"RelevancePropagation.is_compatible","text":"is_compatible(rule, layer)\n\nCheck compatibility of a LRP-Rule with layer type.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Compatibility settings:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"LRP_CONFIG.supports_layer\nLRP_CONFIG.supports_activation","category":"page"},{"location":"api/#RelevancePropagation.LRP_CONFIG.supports_layer","page":"API Reference","title":"RelevancePropagation.LRP_CONFIG.supports_layer","text":"LRP_CONFIG.supports_layer(layer)\n\nCheck whether LRP can be used on a layer or a Chain. To extend LRP to your own layers, define:\n\nLRP_CONFIG.supports_layer(::MyLayer) = true          # for structs\nLRP_CONFIG.supports_layer(::typeof(mylayer)) = true  # for functions\n\n\n\n\n\n","category":"function"},{"location":"api/#RelevancePropagation.LRP_CONFIG.supports_activation","page":"API Reference","title":"RelevancePropagation.LRP_CONFIG.supports_activation","text":"LRP_CONFIG.supports_activation(σ)\n\nCheck whether LRP can be used on a given activation function. To extend LRP to your own activation functions, define:\n\nLRP_CONFIG.supports_activation(::typeof(myactivation)) = true  # for functions\nLRP_CONFIG.supports_activation(::MyActivation) = true          # for structs\n\n\n\n\n\n","category":"function"},{"location":"api/#CRP","page":"API Reference","title":"CRP","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"CRP\nTopNFeatures\nIndexedFeatures","category":"page"},{"location":"api/#RelevancePropagation.CRP","page":"API Reference","title":"RelevancePropagation.CRP","text":"CRP(lrp_analyzer, layer, features)\n\nUse Concept Relevance Propagation to explain the output of a neural network with respect to specific features in a given layer.\n\nArguments\n\nlrp_analyzer::LRP: LRP analyzer\nlayer::Int: Index of layer after which the concept is located\nfeatures: Concept / feature to explain.\n\nSee also TopNFeatures and IndexedFeatures.\n\nReferences\n\n[1] R. Achtibat et al., From attribution maps to human-understandable explanations     through Concept Relevance Propagation\n\n\n\n\n\n","category":"type"},{"location":"api/#XAIBase.TopNFeatures","page":"API Reference","title":"XAIBase.TopNFeatures","text":"TopNFeatures(n)\n\nSelect top-n features.\n\nFor outputs of convolutional layers, the relevance is summed across height and width channels for each feature.\n\nSee also IndexedFeatures.\n\n\n\n\n\n","category":"type"},{"location":"api/#XAIBase.IndexedFeatures","page":"API Reference","title":"XAIBase.IndexedFeatures","text":"IndexedFeatures(indices...)\n\nSelect features by indices.\n\nFor outputs of convolutional layers, the index refers to a feature dimension.\n\nSee also See also TopNFeatures.\n\n\n\n\n\n","category":"type"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"developer/#developer","page":"Developer Documentation","title":"Developer Documentation","text":"","category":"section"},{"location":"developer/#Generic-LRP-rule-implementation","page":"Developer Documentation","title":"Generic LRP rule implementation","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Before we dive into package-specific implementation details  in later sections of this developer documentation,  we first need to cover some fundamentals of LRP, starting with our notation.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"The generic LRP rule, of which the 0-, epsilon- and gamma-rules are special cases, reads[1][2]","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"beginequation\nR_j^k = sum_i fracrho(W_ij)  a_j^kepsilon + sum_l rho(W_il)  a_l^k + rho(b_i) R_i^k+1\nendequation","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"where ","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"W is the weight matrix of the layer\nb is the bias vector of the layer\na^k is the activation vector at the input of layer k\na^k+1 is the activation vector at the output of layer k\nR^k is the relevance vector at the input of layer k\nR^k+1 is the relevance vector at the output of layer k\nrho is a function that modifies parameters (what we call modify_parameters)\nepsilon is a small positive constant to avoid division by zero","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Subscript characters are used to index vectors and matrices  (e.g. b_i is the i-th entry of the bias vector),  while the superscripts ^k and ^k+1  indicate the relative positions of activations a and relevances R in the model. For any k, a^k and R^k have the same shape. ","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Note that every term in this equation is a scalar value, which removes the need to differentiate between matrix and element-wise operations.","category":"page"},{"location":"developer/#Linear-layers","page":"Developer Documentation","title":"Linear layers","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"LRP was developed for deep rectifier networks, neural networks that are composed of linear layers with ReLU activation functions. Linear layers are layers that can be represented as affine transformations of the form ","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"beginequation\nf(x) = Wx + b quad \nendequation","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"This includes most commonly used types of layers, such as fully connected layers,  convolutional layers, pooling layers, and normalization layers.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"We will now describe a generic implementation of equation (1)  that can be applied to any linear layer.","category":"page"},{"location":"developer/#fallback","page":"Developer Documentation","title":"The automatic differentiation fallback","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"The computation of the generic LRP rule can be decomposed into four steps[1]:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"beginarraylr\nz_i = sum_l rho(W_il)  a_l^k + rho(b_i)  text(Step 1) 05em\ns_i = R_i^k+1  (z_i + epsilon)            text(Step 2) 05em\nc_j = sum_i rho(W_ij)  s_i                text(Step 3) 05em\nR_j^k = a_j^k c_j                         text(Step 4)\nendarray","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"To compute step 1, we first create a modified layer,  applying rho to the weights and biases  and replacing the activation function with the identity function. The vector z is then computed using a forward pass through the modified layer. It has the same dimensionality as R^k+1 and a^k+1.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Step 2 is an element-wise division of R^k+1 by z. To avoid division by zero, a small constant epsilon is added to z when necessary.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Step 3 is trivial for fully connected layers,  as rho(W) corresponds to the weight matrix of the modified layer. For other types of linear layers, however, the implementation is more involved: A naive approach would be to construct a large matrix W that corresponds to the affine transformation Wx+b implemented by the modified layer. This has multiple drawbacks:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"the implementation is error-prone\na separate implementation is required for each type of linear layer\nfor some layer types, e.g. pooling layers, the matrix W depends on the input\nfor many layer types, e.g. convolutional layers,  the matrix W is very large and sparse, mostly consisting of zeros, leading to a large computational overhead","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"A better approach can be found by observing that the matrix W is the Jacobian of the affine transformation f(x) = Wx + b. The vector c computed in step 3 corresponds to c = s^T W, a so-called Vector-Jacobian-Product (VJP) of the vector s with the Jacobian W. ","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"VJPs are the fundamental building blocks of reverse-mode automatic differentiation (AD), and therefore implemented by most AD frameworks in a highly performant, matrix-free, GPU-accelerated manner. Note that computing the VJP is much more efficient than first computing the full Jacobian W and later multiplying it with s.  This is due to the fact that computing the full Jacobian of a function  f mathbbR^n rightarrow mathbbR^m requires computing m VJPs.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Functions that compute VJP's are commonly called pullbacks. Using the Zygote.jl AD system, we obtain the output z of a modified layer and its pullback back in a single function call:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"z, back = Zygote.pullback(modified_layer, aᵏ)","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"We then call the pullback with the vector s to obtain c:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"c = back(s)","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Finally, step 4 consists of an element-wise multiplication of the vector c  with the input activation vector a^k, resulting in the relevance vector R^k.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"This AD-based implementation is used in RelevancePropagation.jl as the default method for all layer types that don't have a more optimized implementation (e.g. fully connected layers). We will refer to it as the \"AD fallback\".","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"For more background information on automatic differentiation, refer to the  JuML lecture on AD.","category":"page"},{"location":"developer/#LRP-analyzer-struct","page":"Developer Documentation","title":"LRP analyzer struct","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"The LRP analyzer struct holds three fields: the model to analyze, the LRP rules to use, and pre-allocated modified_layers.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"As described in the section on Composites, applying a composite to a model will return LRP rules in nested ChainTuple, ParallelTuple and SkipConnectionTuples. These wrapper types are used to match the structure of Flux models with Chain,  Parallel and SkipConnection layers while avoiding type piracy.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"When creating an LRP analyzer with the default keyword argument flatten=true,  flatten_model is called on the model and rules. This is done for performance reasons, as discussed in  Flattening the model.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"After passing the Model checks, modified layers are pre-allocated, once again using the ChainTuple, ParallelTuple  and SkipConnectionTuple wrapper types to match the structure of the model. If a rule doesn't modify a layer,  the corresponding entry in modified_layers is set to nothing,  avoiding unnecessary allocations.  If a rule requires multiple modified layers,  the corresponding entry in modified_layers is set to a named tuple of modified layers. Apart from these special cases,  the corresponding entry in modified_layers is simply set to the modified layer.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"For a detailed description of the layer modification mechanism, refer to the section on Advanced layer modification.","category":"page"},{"location":"developer/#Forward-and-reverse-pass","page":"Developer Documentation","title":"Forward and reverse pass","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"When calling an LRP analyzer, a forward pass through the model is performed, saving the activations aᵏ for all layers k in a vector called as. This vector of activations is then used to pre-allocate the relevances R^k  for all layers in a vector called Rs. This is possible since for any layer k, a^k and R^k have the same shape. Finally, the last array of relevances R^N in Rs is set to zeros,  except for the specified output neuron, which is set to one.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"We can now run the reverse pass, iterating backwards over the layers in the model and writing relevances R^k into the pre-allocated array Rs:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"for k in length(model):-1:1\n    #                  └─ loop over layers in reverse\n    lrp!(Rs[k], rules[k], layers[k], modified_layers[k], as[k], Rs[k+1])\n    #    └─ Rᵏ: modified in-place                        └─ aᵏ  └─ Rᵏ⁺¹\nend","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"This is done by calling low-level functions","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"function lrp!(Rᵏ, rule, layer, modified_layer, aᵏ, Rᵏ⁺¹)\n    Rᵏ .= ...\nend","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"that implement individual LRP rules. The correct rule is applied via  multiple dispatch on the types of the arguments rule and modified_layer. The relevance Rᵏ is then computed based on the input activation aᵏ and the output relevance Rᵏ⁺¹.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"The exclamation point in the function name lrp! is a  naming convention in Julia to denote functions that modify their arguments –  in this case the first argument Rs[k], which corresponds to R^k.","category":"page"},{"location":"developer/#Rule-calls","page":"Developer Documentation","title":"Rule calls","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"As discussed in The AD fallback, the default LRP fallback for unknown layers uses AD via  Zygote. Now that you are familiar with both the API and the four-step computation of the generic LRP rules, the following implementation should be straightforward to understand:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"function lrp!(Rᵏ, rule, layer, modified_layer, aᵏ, Rᵏ⁺¹)\n   # Use modified_layer if available\n   layer = isnothing(modified_layer) ? layer : modified_layer\n\n   ãᵏ = modify_input(rule, aᵏ)\n   z, back = Zygote.pullback(modified_layer, ãᵏ)\n   s = Rᵏ⁺¹ ./ modify_denominator(rule, z)\n   Rᵏ .= ãᵏ .* only(back(s))\nend","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Not only lrp! dispatches on the rule and layer type,  but also the internal functions modify_input and modify_denominator. Unknown layers that are registered in the LRP_CONFIG use this exact function.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"All LRP rules are implemented in the file /src/rules.jl.","category":"page"},{"location":"developer/#Specialized-implementations","page":"Developer Documentation","title":"Specialized implementations","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"In other programming languages, LRP is commonly implemented in an object-oriented manner, providing a single backward pass implementation per rule. This can be seen as a form of single dispatch on the rule type.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Using multiple dispatch, we can implement specialized versions of lrp! that not only take into account the rule type, but also the layer type,  for example for fully connected layers or reshaping layers. ","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Reshaping layers don't affect attributions. We can therefore avoid the computational overhead of AD by writing a specialized implementation that simply reshapes back:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"function lrp!(Rᵏ, rule, layer::ReshapingLayer, modified_layer, aᵏ, Rᵏ⁺¹)\n    Rᵏ .= reshape(Rᵏ⁺¹, size(aᵏ))\nend","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"We can even provide a specialized implementation of the generic LRP rule for Dense layers. Since we can access the weight matrix directly, we can skip the use of automatic differentiation and implement the following equation directly, using Einstein summation notation:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"R_j^k = sum_i fracrho(W_ij)  a_j^kepsilon + sum_l rho(W_il)  a_l^k + rho(b_i) R_i^k+1","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"function lrp!(Rᵏ, rule, layer::Dense, modified_layer, aᵏ, Rᵏ⁺¹)\n   # Use modified_layer if available\n   layer = isnothing(modified_layer) ? layer : modified_layer\n\n   ãᵏ = modify_input(rule, aᵏ)\n   z = modify_denominator(rule, layer(ãᵏ))\n\n   # Implement LRP using Einsum notation, where `b` is the batch index\n   @tullio Rᵏ[j, b] = layer.weight[i, j] * ãᵏ[j, b] / z[i, b] * Rᵏ⁺¹[i, b]\nend","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"For maximum low-level control beyond modify_input and modify_denominator, you can also implement your own lrp! function and dispatch on individual rule types MyRule and layer types MyLayer:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"function lrp!(Rᵏ, rule::MyRule, layer::MyLayer, modified_layer, aᵏ, Rᵏ⁺¹)\n    Rᵏ .= ...\nend","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"[1]: G. Montavon et al., Layer-Wise Relevance Propagation: An Overview","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"[2]: W. Samek et al., Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"EditURL = \"../literate/composites.jl\"","category":"page"},{"location":"generated/composites/#composites","page":"Assigning Rules to Layers","title":"Assigning LRP Rules to Layers","text":"","category":"section"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"In this example, we will show how to assign LRP rules to specific layers. For this purpose, we first define a small VGG-like convolutional neural network:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"using RelevancePropagation\nusing Flux\n\nmodel = Chain(\n    Chain(\n        Conv((3, 3), 3 => 8, relu; pad=1),\n        Conv((3, 3), 8 => 8, relu; pad=1),\n        MaxPool((2, 2)),\n        Conv((3, 3), 8 => 16, relu; pad=1),\n        Conv((3, 3), 16 => 16, relu; pad=1),\n        MaxPool((2, 2)),\n    ),\n    Chain(Flux.flatten, Dense(1024 => 512, relu), Dropout(0.5), Dense(512 => 100, relu)),\n);\nnothing #hide","category":"page"},{"location":"generated/composites/#composites-manual","page":"Assigning Rules to Layers","title":"Manually assigning rules","text":"","category":"section"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"When creating an LRP-analyzer, we can assign individual rules to each layer. As we can see above, our model is a Chain of two Flux Chains. Using flatten_model, we can flatten the model into a single Chain:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"model_flat = flatten_model(model)","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"This allows us to define an LRP analyzer using an array of rules matching the length of the Flux chain:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"rules = [\n    FlatRule(),\n    ZPlusRule(),\n    ZeroRule(),\n    ZPlusRule(),\n    ZPlusRule(),\n    ZeroRule(),\n    PassRule(),\n    EpsilonRule(),\n    PassRule(),\n    EpsilonRule(),\n];\nnothing #hide","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"The LRP analyzer will show a summary of how layers and rules got matched:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"LRP(model_flat, rules)","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"However, this approach only works for models that can be fully flattened. For unflattened models and models containing Parallel and SkipConnection layers, we can compose rules using ChainTuple, ParallelTuple and SkipConnectionTuples which match the model structure:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"rules = ChainTuple(\n    ChainTuple(FlatRule(), ZPlusRule(), ZeroRule(), ZPlusRule(), ZPlusRule(), ZeroRule()),\n    ChainTuple(PassRule(), EpsilonRule(), PassRule(), EpsilonRule()),\n)\n\nanalyzer = LRP(model, rules; flatten=false)","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"note: Keyword argument `flatten`\nWe used the LRP keyword argument flatten=false to showcase that the structure of the model can be preserved. For performance reasons, the default flatten=true is recommended.","category":"page"},{"location":"generated/composites/#Custom-composites","page":"Assigning Rules to Layers","title":"Custom composites","text":"","category":"section"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"Instead of manually defining a list of rules, we can also define a Composite. A composite constructs a list of LRP-rules by sequentially applying the composite primitives it contains.","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"To obtain the same set of rules as in the previous example, we can define","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"composite = Composite(\n    GlobalTypeMap( # the following maps of layer types to LRP rules are applied globally\n        Conv                 => ZPlusRule(),   # apply ZPlusRule on all Conv layers\n        Dense                => EpsilonRule(), # apply EpsilonRule on all Dense layers\n        Dropout              => PassRule(),    # apply PassRule on all Dropout layers\n        MaxPool              => ZeroRule(),    # apply ZeroRule on all MaxPool layers\n        typeof(Flux.flatten) => PassRule(),    # apply PassRule on all flatten layers\n    ),\n    FirstLayerMap( # the following rule is applied to the first layer\n        FlatRule(),\n    ),\n);\nnothing #hide","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"We now construct an LRP analyzer from composite","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"analyzer = LRP(model, composite; flatten=false)","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"As you can see, this analyzer contains the same rules as our previous one. To compute rules for a model without creating an analyzer, use lrp_rules:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"lrp_rules(model, composite)","category":"page"},{"location":"generated/composites/#Composite-primitives","page":"Assigning Rules to Layers","title":"Composite primitives","text":"","category":"section"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"The following Composite primitives can used to construct a Composite.","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"To apply a single rule, use:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"LayerMap to apply a rule to a layer at a given index\nGlobalMap to apply a rule to all layers\nRangeMap to apply a rule to a positional range of layers\nFirstLayerMap to apply a rule to the first layer\nLastLayerMap to apply a rule to the last layer","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"To apply a set of rules to layers based on their type, use:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"GlobalTypeMap to apply a dictionary that maps layer types to LRP-rules\nRangeTypeMap for a TypeMap on generalized ranges\nFirstLayerTypeMap for a TypeMap on the first layer of a model\nLastLayerTypeMap for a TypeMap on the last layer\nFirstNTypeMap for a TypeMap on the first n layers","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"Primitives are called sequentially in the order the Composite was created with and overwrite rules specified by previous primitives.","category":"page"},{"location":"generated/composites/#Assigning-a-rule-to-a-specific-layer","page":"Assigning Rules to Layers","title":"Assigning a rule to a specific layer","text":"","category":"section"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"To assign a rule to a specific layer, we can use LayerMap, which maps an LRP-rule to all layers in the model at the given index.","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"To display indices, use the show_layer_indices helper function:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"show_layer_indices(model)","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"Let's demonstrate LayerMap by assigning a specific rule to the last Conv layer at index (1, 5):","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"composite = Composite(LayerMap((1, 5), EpsilonRule()))\n\nLRP(model, composite; flatten=false)","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"This approach also works with Parallel layers.","category":"page"},{"location":"generated/composites/#composites-presets","page":"Assigning Rules to Layers","title":"Composite presets","text":"","category":"section"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"RelevancePropagation.jl provides a set of default composites. A list of all implemented default composites can be found in the API reference, e.g. the EpsilonPlusFlat composite:","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"composite = EpsilonPlusFlat()","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"analyzer = LRP(model, composite; flatten=false)","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"","category":"page"},{"location":"generated/composites/","page":"Assigning Rules to Layers","title":"Assigning Rules to Layers","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"EditURL = \"../literate/crp.jl\"","category":"page"},{"location":"generated/crp/#Concept-Relevance-Propagation","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"","category":"section"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"In From attribution maps to human-understandable explanations through Concept Relevance Propagation (CRP), Achtibat et al. propose the conditioning of LRP relevances on individual features of a model.","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"note: Note\nThis package is part the Julia-XAI ecosystem and builds on the basics shown in the Getting started guide.","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"We start out by loading the same pre-trained LeNet5 model and MNIST input data:","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"using RelevancePropagation\nusing Flux\n\nusing BSON # hide\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model] # load pre-trained LeNet-5 model","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nconvert2image(MNIST, x)","category":"page"},{"location":"generated/crp/#Step-1:-Create-LRP-analyzer","page":"Concept Relevance Propagation","title":"Step 1: Create LRP analyzer","text":"","category":"section"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"To create a CRP analyzer, first define an LRP analyzer with your desired rules:","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"composite = EpsilonPlusFlat()\nlrp_analyzer = LRP(model, composite)","category":"page"},{"location":"generated/crp/#Step-2:-Define-concepts","page":"Concept Relevance Propagation","title":"Step 2: Define concepts","text":"","category":"section"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"Then, specify the index of the layer on the outputs of which you want to condition the explanation. In this example, we are interested in the outputs of the last convolutional layer, layer 3:","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"feature_layer = 3    # index of relevant layer in model\nmodel[feature_layer] # show layer","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"Then, specify the concepts / features you are interested in. To automatically select the n most relevant features, use TopNFeatures.","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"Note that for convolutional layers, a feature corresponds to an entire output channel of the layer.","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"features = TopNFeatures(5)","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"To manually specify features, use IndexedFeatures.","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"features = IndexedFeatures(1, 2, 10)","category":"page"},{"location":"generated/crp/#Step-3:-Use-CRP-analyzer","page":"Concept Relevance Propagation","title":"Step 3: Use CRP analyzer","text":"","category":"section"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"We can now create a CRP analyzer and use it like any other analyzer from RelevancePropagation.jl:","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"analyzer = CRP(lrp_analyzer, feature_layer, features)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/crp/#Using-CRP-on-input-batches","page":"Concept Relevance Propagation","title":"Using CRP on input batches","text":"","category":"section"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"Note that CRP uses the batch dimension to return explanations. When using CRP on batches, the explanations are first sorted by features, then inputs, e.g. [c1_i1, c1_i2, c2_i1, c2_i2, c3_i1, c3_i2] in the following example:","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"x, y = MNIST(Float32, :test)[10:11]\nbatch = reshape(x, 28, 28, 1, :)\n\nheatmap(batch, analyzer)","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"","category":"page"},{"location":"generated/crp/","page":"Concept Relevance Propagation","title":"Concept Relevance Propagation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"EditURL = \"../literate/custom_rules.jl\"","category":"page"},{"location":"generated/custom_rules/#custom-rules","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"","category":"section"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"One of the design goals of RelevancePropagation.jl is to combine ease of use and extensibility for the purpose of research.","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"This example will show you how to implement custom LRP rules.","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"note: Note\nThis package is part the Julia-XAI ecosystem and builds on the basics shown in the Getting started guide.","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"We start out by loading the same pre-trained LeNet-5 model and MNIST input data:","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"using RelevancePropagation\nusing Flux\nusing MLDatasets\nusing ImageCore\nusing BSON\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model] # load pre-trained LeNet-5 model","category":"page"},{"location":"generated/custom_rules/#Implementing-a-custom-rule","page":"Custom LRP Rules","title":"Implementing a custom rule","text":"","category":"section"},{"location":"generated/custom_rules/#Step-1:-Define-rule-struct","page":"Custom LRP Rules","title":"Step 1: Define rule struct","text":"","category":"section"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"Let's define a rule that modifies the weights and biases of our layer on the forward pass. The rule has to be of supertype AbstractLRPRule.","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"struct MyGammaRule <: AbstractLRPRule end","category":"page"},{"location":"generated/custom_rules/#Step-2:-Implement-rule-behavior","page":"Custom LRP Rules","title":"Step 2: Implement rule behavior","text":"","category":"section"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"It is then possible to dispatch on the following four utility functions with the rule type MyCustomLRPRule to define custom rules without writing boilerplate code.","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"modify_input(rule::MyGammaRule, input)\nmodify_parameters(rule::MyGammaRule, parameter)\nmodify_denominator(rule::MyGammaRule, denominator)\nis_compatible(rule::MyGammaRule, layer)","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"By default:","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"modify_input doesn't change the input\nmodify_parameters doesn't change the parameters\nmodify_denominator avoids division by zero by adding a small epsilon-term (1.0f-9)\nis_compatible returns true if a layer has fields weight and bias","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"To extend internal functions, import them explicitly:","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"import RelevancePropagation: modify_parameters\n\nmodify_parameters(::MyGammaRule, param) = param + 0.25f0 * relu.(param)","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"Note that we didn't implement three of the four functions. This is because the defaults are sufficient to implement the GammaRule.","category":"page"},{"location":"generated/custom_rules/#Step-3:-Use-rule-in-LRP-analyzer","page":"Custom LRP Rules","title":"Step 3: Use rule in LRP analyzer","text":"","category":"section"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"We can directly use our rule to make an analyzer!","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"rules = [\n    ZPlusRule(),\n    EpsilonRule(),\n    MyGammaRule(), # our custom GammaRule\n    EpsilonRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n]\nanalyzer = LRP(model, rules)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"We just implemented our own version of the γ-rule in 2 lines of code. The heatmap perfectly matches the pre-implemented GammaRule:","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"rules = [\n    ZPlusRule(),\n    EpsilonRule(),\n    GammaRule(), # XAI.jl's GammaRule\n    EpsilonRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n]\nanalyzer = LRP(model, rules)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/custom_rules/#Performance-tips","page":"Custom LRP Rules","title":"Performance tips","text":"","category":"section"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"Make sure functions like modify_parameters don't promote the type of weights (e.g. from Float32 to Float64).\nIf your rule MyRule doesn't modify weights or biases, defining modify_layer(::MyRule, layer) = nothing can provide reduce memory allocations and improve performance.","category":"page"},{"location":"generated/custom_rules/#custom-rules-advanced","page":"Custom LRP Rules","title":"Advanced layer modification","text":"","category":"section"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"For more granular control over weights and biases, modify_weight and modify_bias can be used.","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"If the layer doesn't use weights (layer.weight) and biases (layer.bias), RelevancePropagation provides a lower-level variant of modify_parameters called modify_layer. This function is expected to take a layer and return a new, modified layer. To add compatibility checks between rule and layer types, extend is_compatible.","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"warning: Extending modify_layer\nUse of a custom function modify_layer will overwrite functionality of modify_parameters, modify_weight and modify_bias for the implemented combination of rule and layer types. This is due to the fact that internally, modify_weight and modify_bias are called by the default implementation of modify_layer. modify_weight and modify_bias in turn call modify_parameters by default.The default call structure looks as follows:┌─────────────────────────────────────────┐\n│              modify_layer               │\n└─────────┬─────────────────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│   modify_weight   │ │    modify_bias    │\n└─────────┬─────────┘ └─────────┬─────────┘\n          │ calls               │ calls\n┌─────────▼─────────┐ ┌─────────▼─────────┐\n│ modify_parameters │ │ modify_parameters │\n└───────────────────┘ └───────────────────┘Therefore modify_layer should only be extended for a specific rule and a specific layer type.","category":"page"},{"location":"generated/custom_rules/#Advanced-LRP-rules","page":"Custom LRP Rules","title":"Advanced LRP rules","text":"","category":"section"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"To implement custom LRP rules that require more than modify_layer, modify_input and modify_denominator, take a look at the LRP developer documentation.","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"","category":"page"},{"location":"generated/custom_rules/","page":"Custom LRP Rules","title":"Custom LRP Rules","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = RelevancePropagation","category":"page"},{"location":"#RelevancePropagation.jl","page":"Home","title":"RelevancePropagation.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Julia implementation of Layerwise Relevance Propagation (LRP)  and Concept Relevance Propagation (CRP)  for use with Flux.jl models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThis package is part the Julia-XAI ecosystem. For an introduction to the ecosystem, please refer to the  Getting started guide.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install this package and its dependencies, open the Julia REPL and run ","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add RelevancePropagation","category":"page"},{"location":"#Manual","page":"Home","title":"Manual","text":"","category":"section"},{"location":"#Basic-Usage","page":"Home","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"generated/basics.md\",\n    \"generated/composites.md\",\n    \"generated/crp.md\",\n]\nDepth = 3","category":"page"},{"location":"#Advanced-Usage","page":"Home","title":"Advanced Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"generated/custom_layer.md\",\n    \"generated/custom_rules.md\",\n    \"developer.md\",\n]\nDepth = 3","category":"page"},{"location":"#LRP-Rule-Overview","page":"Home","title":"LRP Rule Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"rules.md\"]\nDepth = 3","category":"page"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"api.md\"]\nDepth = 3","category":"page"},{"location":"rules/#rules","page":"LRP Rule Overview","title":"LRP Rule Overview","text":"","category":"section"},{"location":"rules/#Notation","page":"LRP Rule Overview","title":"Notation","text":"","category":"section"},{"location":"rules/","page":"LRP Rule Overview","title":"LRP Rule Overview","text":"We use the following notation for LRP rules: ","category":"page"},{"location":"rules/","page":"LRP Rule Overview","title":"LRP Rule Overview","text":"W is the weight matrix of the layer\nb is the bias vector of the layer\na^k is the activation vector at the input of layer k\na^k+1 is the activation vector at the output of layer k\nR^k is the relevance vector at the input of layer k\nR^k+1 is the relevance vector at the output of layer k\nrho is a function that modifies parameters (what we call modify_parameters)\nepsilon is a small positive constant to avoid division by zero","category":"page"},{"location":"rules/","page":"LRP Rule Overview","title":"LRP Rule Overview","text":"Subscript characters are used to index vectors and matrices  (e.g. b_i is the i-th entry of the bias vector),  while the superscripts ^k and ^k+1 indicate the relative positions  of activations a and relevances R in the model. For any k, a^k and R^k have the same shape. ","category":"page"},{"location":"rules/","page":"LRP Rule Overview","title":"LRP Rule Overview","text":"Note that all terms in the following equations are scalar value, which removes the need to differentiate between matrix and element-wise operations. For more information, refer to the developer documentation.","category":"page"},{"location":"rules/#Basic-rules","page":"LRP Rule Overview","title":"Basic rules","text":"","category":"section"},{"location":"rules/","page":"LRP Rule Overview","title":"LRP Rule Overview","text":"ZeroRule\nEpsilonRule\nPassRule","category":"page"},{"location":"rules/#RelevancePropagation.ZeroRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.ZeroRule","text":"ZeroRule()\n\nLRP-0 rule. Commonly used on upper layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_i fracW_ija_j^ksum_l W_ila_l^k+b_i R_i^k+1\n\nReferences\n\nS. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation\n\n\n\n\n\n","category":"type"},{"location":"rules/#RelevancePropagation.EpsilonRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.EpsilonRule","text":"EpsilonRule([epsilon=1.0e-6])\n\nLRP-ϵ rule. Commonly used on middle layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifracW_ija_j^kepsilon +sum_lW_ila_l^k+b_i R_i^k+1\n\nOptional arguments\n\nepsilon: Optional stabilization parameter, defaults to 1.0e-6.\n\nReferences\n\nS. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation\n\n\n\n\n\n","category":"type"},{"location":"rules/#RelevancePropagation.PassRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.PassRule","text":"PassRule()\n\nPass-through rule. Passes relevance through to the lower layer.\n\nSupports layers with constant input and output shapes, e.g. reshaping layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = R_j^k+1\n\n\n\n\n\n","category":"type"},{"location":"rules/#Lower-layer-rules","page":"LRP Rule Overview","title":"Lower layer rules","text":"","category":"section"},{"location":"rules/","page":"LRP Rule Overview","title":"LRP Rule Overview","text":"GammaRule\nAlphaBetaRule\nZPlusRule","category":"page"},{"location":"rules/#RelevancePropagation.GammaRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.GammaRule","text":"GammaRule([gamma=0.25])\n\nLRP-γ rule. Commonly used on lower layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifrac(W_ij+gamma W_ij^+)a_j^k\n    sum_l(W_il+gamma W_il^+)a_l^k+(b_i+gamma b_i^+) R_i^k+1\n\nOptional arguments\n\ngamma: Optional multiplier for added positive weights, defaults to 0.25.\n\nReferences\n\nG. Montavon et al., Layer-Wise Relevance Propagation: An Overview\n\n\n\n\n\n","category":"type"},{"location":"rules/#RelevancePropagation.AlphaBetaRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.AlphaBetaRule","text":"AlphaBetaRule([alpha=2.0, beta=1.0])\n\nLRP-αβ rule. Weights positive and negative contributions according to the parameters alpha and beta respectively. The difference α-β must be equal to one. Commonly used on lower layers.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ileft(\n    alphafracleft(W_ija_j^kright)^+sum_lleft(W_ila_l^k+b_iright)^+\n    -betafracleft(W_ija_j^kright)^-sum_lleft(W_ila_l^k+b_iright)^-\nright) R_i^k+1\n\nOptional arguments\n\nalpha: Multiplier for the positive output term, defaults to 2.0.\nbeta: Multiplier for the negative output term, defaults to 1.0.\n\nReferences\n\nS. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation\nG. Montavon et al., Layer-Wise Relevance Propagation: An Overview\n\n\n\n\n\n","category":"type"},{"location":"rules/#RelevancePropagation.ZPlusRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.ZPlusRule","text":"ZPlusRule()\n\nLRP-z rule. Commonly used on lower layers.\n\nEquivalent to AlphaBetaRule(1.0f0, 0.0f0), but slightly faster. See also AlphaBetaRule.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifracleft(W_ija_j^kright)^+sum_lleft(W_ila_l^k+b_iright)^+ R_i^k+1\n\nReferences\n\nS. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation\nG. Montavon et al., Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition\n\n\n\n\n\n","category":"type"},{"location":"rules/#Input-layer-rules","page":"LRP Rule Overview","title":"Input layer rules","text":"","category":"section"},{"location":"rules/","page":"LRP Rule Overview","title":"LRP Rule Overview","text":"FlatRule\nWSquareRule\nZBoxRule","category":"page"},{"location":"rules/#RelevancePropagation.FlatRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.FlatRule","text":"FlatRule()\n\nLRP-Flat rule. Similar to the WSquareRule, but with all weights set to one and all bias terms set to zero.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifrac1sum_l 1 R_i^k+1 = sum_ifrac1n_i R_i^k+1\n\nwhere n_i is the number of input neurons connected to the output neuron at index i.\n\nReferences\n\nS. Lapuschkin et al., Unmasking Clever Hans predictors and assessing what machines really learn\n\n\n\n\n\n","category":"type"},{"location":"rules/#RelevancePropagation.WSquareRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.WSquareRule","text":"WSquareRule()\n\nLRP-w² rule. Commonly used on the first layer when values are unbounded.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifracW_ij^2sum_l W_il^2 R_i^k+1\n\nReferences\n\nG. Montavon et al., Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition\n\n\n\n\n\n","category":"type"},{"location":"rules/#RelevancePropagation.ZBoxRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.ZBoxRule","text":"ZBoxRule(low, high)\n\nLRP-zᴮ-rule. Commonly used on the first layer for pixel input.\n\nThe parameters low and high should be set to the lower and upper bounds of the input features, e.g. 0.0 and 1.0 for raw image data. It is also possible to provide two arrays of that match the input size.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k=sum_i fracW_ija_j^k - W_ij^+l_j - W_ij^-h_j\n    sum_l W_ila_l^k+b_i - left(W_il^+l_l+b_i^+right) - left(W_il^-h_l+b_i^-right) R_i^k+1\n\nReferences\n\nG. Montavon et al., Layer-Wise Relevance Propagation: An Overview\n\n\n\n\n\n","category":"type"},{"location":"rules/#Specialized-rules","page":"LRP Rule Overview","title":"Specialized rules","text":"","category":"section"},{"location":"rules/","page":"LRP Rule Overview","title":"LRP Rule Overview","text":"GeneralizedGammaRule","category":"page"},{"location":"rules/#RelevancePropagation.GeneralizedGammaRule-rules","page":"LRP Rule Overview","title":"RelevancePropagation.GeneralizedGammaRule","text":"GeneralizedGammaRule([gamma=0.25])\n\nGeneralized LRP-γ rule. Can be used on layers with leakyrelu activation functions.\n\nDefinition\n\nPropagates relevance R^k+1 at layer output to R^k at layer input according to\n\nR_j^k = sum_ifrac\n    (W_ij+gamma W_ij^+)a_j^+ +(W_ij+gamma W_ij^-)a_j^-\n    sum_l(W_il+gamma W_il^+)a_j^+ +(W_il+gamma W_il^-)a_j^- +(b_i+gamma b_i^+)\nI(z_k0) cdot R^k+1_i\n+sum_ifrac\n    (W_ij+gamma W_ij^-)a_j^+ +(W_ij+gamma W_ij^+)a_j^-\n    sum_l(W_il+gamma W_il^-)a_j^+ +(W_il+gamma W_il^+)a_j^- +(b_i+gamma b_i^-)\nI(z_k0) cdot R^k+1_i\n\nOptional arguments\n\ngamma: Optional multiplier for added positive weights, defaults to 0.25.\n\nReferences\n\nL. Andéol et al., Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization\n\n\n\n\n\n","category":"type"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"EditURL = \"../literate/basics.jl\"","category":"page"},{"location":"generated/basics/#Creating-an-LRP-Analyzer","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"","category":"section"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"note: Note\nThis package is part the Julia-XAI ecosystem. For an introduction to the ecosystem, please refer to the Getting started guide.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"We start out by loading a small convolutional neural network:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"using RelevancePropagation\nusing Flux\n\nmodel = Chain(\n    Chain(\n        Conv((3, 3), 3 => 8, relu; pad=1),\n        Conv((3, 3), 8 => 8, relu; pad=1),\n        MaxPool((2, 2)),\n        Conv((3, 3), 8 => 16; pad=1),\n        BatchNorm(16, relu),\n        Conv((3, 3), 16 => 8, relu; pad=1),\n        BatchNorm(8, relu),\n    ),\n    Chain(Flux.flatten, Dense(2048 => 512, relu), Dropout(0.5), Dense(512 => 100, softmax)),\n);\nnothing #hide","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"This model contains two chains: the convolutional layers and the fully connected layers.","category":"page"},{"location":"generated/basics/#Model-preparation","page":"Creating an LRP Analyzer","title":"Model preparation","text":"","category":"section"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"note: TLDR\nUse strip_softmax to strip the output softmax from your model. Otherwise model checks will fail.\nUse canonize to fuse linear layers.\nDon't just call LRP(model), instead use a Composite to apply LRP rules to your model. Read Assigning rules to layers for more information.\nBy default, LRP will call flatten_model to flatten your model. This reduces computational overhead.","category":"page"},{"location":"generated/basics/#Stripping-the-output-softmax","page":"Creating an LRP Analyzer","title":"Stripping the output softmax","text":"","category":"section"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"When using LRP, it is recommended to explain output logits instead of probabilities. This can be done by stripping the output softmax activation from the model using the strip_softmax function:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"model = strip_softmax(model)","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"If you don't remove the output softmax, model checks will fail.","category":"page"},{"location":"generated/basics/#canonization","page":"Creating an LRP Analyzer","title":"Model canonization","text":"","category":"section"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"LRP is not invariant to a model's implementation. Applying the GammaRule to two linear layers in a row will yield different results than first fusing the two layers into one linear layer and then applying the rule. This fusing is called \"canonization\" and can be done using the canonize function:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"model_canonized = canonize(model)","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"After canonization, the first BatchNorm layer has been fused into the preceding Conv layer. The second BatchNorm layer wasn't fused since its preceding Conv layer has a ReLU activation function.","category":"page"},{"location":"generated/basics/#flatten-model","page":"Creating an LRP Analyzer","title":"Flattening the model","text":"","category":"section"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"RelevancePropagation.jl's LRP implementation supports nested Flux Chains and Parallel layers. However, it is recommended to flatten the model before analyzing it.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"LRP is implemented by first running a forward pass through the model, keeping track of the intermediate activations, followed by a backward pass that computes the relevances.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"To keep the LRP implementation simple and maintainable, RelevancePropagation.jl does not pre-compute \"nested\" activations. Instead, for every internal chain, a new forward pass is run to compute activations.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"By \"flattening\" a model, this overhead can be avoided. For this purpose, RelevancePropagation.jl provides the function flatten_model:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"model_flat = flatten_model(model)","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"This function is called by default when creating an LRP analyzer. Note that we pass the unflattened model to the analyzer, but analyzer.model is flattened:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"analyzer = LRP(model)\nanalyzer.model","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"If this flattening is not desired, it can be disabled by passing the keyword argument flatten=false to the LRP constructor.","category":"page"},{"location":"generated/basics/#LRP-rules","page":"Creating an LRP Analyzer","title":"LRP rules","text":"","category":"section"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"The following examples will be run on a pre-trained LeNet-5 model:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"using BSON\n\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model] # load pre-trained LeNet-5 model","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"We also load the MNIST dataset:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"using MLDatasets\nusing ImageCore, ImageIO, ImageShow\n\nindex = 10\nx, y = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :)\n\nconvert2image(MNIST, x)","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"By default, the LRP constructor will assign the ZeroRule to all layers.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"analyzer = LRP(model)","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"This ana lyzer will return heatmaps that look identical to the InputTimesGradient analyzer from ExplainableAI.jl:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"heatmap(input, analyzer)","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"LRP's strength lies in assigning different rules to different layers, based on their functionality in the neural network[1]. RelevancePropagation.jl implements many LRP rules out of the box, but it is also possible to implement custom rules.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"To assign different rules to different layers, use one of the composites presets, or create your own composite, as described in Assigning rules to layers.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"composite = EpsilonPlusFlat() # using composite preset EpsilonPlusFlat","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"analyzer = LRP(model, composite)","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"heatmap(input, analyzer)","category":"page"},{"location":"generated/basics/#Computing-layerwise-relevances","page":"Creating an LRP Analyzer","title":"Computing layerwise relevances","text":"","category":"section"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"If you are interested in computing layerwise relevances, call analyze with an LRP analyzer and the keyword argument layerwise_relevances=true.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"The layerwise relevances can be accessed in the extras field of the returned Explanation:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"expl = analyze(input, analyzer; layerwise_relevances=true)\nexpl.extras.layerwise_relevances","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"Note that the layerwise relevances are only kept for layers in the outermost Chain of the model. Since we used a flattened model, we obtained all relevances.","category":"page"},{"location":"generated/basics/#Performance-tips","page":"Creating an LRP Analyzer","title":"Performance tips","text":"","category":"section"},{"location":"generated/basics/#Using-LRP-with-a-GPU","page":"Creating an LRP Analyzer","title":"Using LRP with a GPU","text":"","category":"section"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"All LRP analyzers support GPU backends, building on top of Flux.jl's GPU support. Using a GPU only requires moving the input array and model weights to the GPU.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"For example, using CUDA.jl:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"using CUDA, cuDNN\nusing Flux\nusing RelevancePropagation\n\n# move input array and model weights to GPU\ninput = input |> gpu # or gpu(input)\nmodel = model |> gpu # or gpu(model)\n\n# analyzers don't require calling `gpu`\nanalyzer = LRP(model)\n\n# explanations are computed on the GPU\nexpl = analyze(input, analyzer)","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"Some operations, like saving, require moving explanations back to the CPU. This can be done using Flux's cpu function:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"val = expl.val |> cpu # or cpu(expl.val)\n\nusing BSON\nBSON.@save \"explanation.bson\" val","category":"page"},{"location":"generated/basics/#Using-LRP-without-a-GPU","page":"Creating an LRP Analyzer","title":"Using LRP without a GPU","text":"","category":"section"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"Using Julia's package extension mechanism, RelevancePropagation.jl's LRP implementation can optionally make use of Tullio.jl and LoopVectorization.jl for faster LRP rules on dense layers.","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"This only requires loading the packages before loading RelevancePropagation.jl:","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"using LoopVectorization, Tullio\nusing RelevancePropagation","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"[1]: G. Montavon et al., Layer-Wise Relevance Propagation: An Overview","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"","category":"page"},{"location":"generated/basics/","page":"Creating an LRP Analyzer","title":"Creating an LRP Analyzer","text":"This page was generated using Literate.jl.","category":"page"}]
}
